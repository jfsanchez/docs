# 游 Apache Hadoop - Instalaci칩n

Instalaci칩n de Apache Hadoop empregando ClusterShell

## Requisitos previos

Estas probas foron feitas nunha contorna baixo *Rocky Linux 8.5 v2* no CESGA, no seu *OpenStack*.

## Grupo de seguridade

Teremos que abrir os seguintes portos (ollo, deber칤amolos abrir s칩 entre eles e para nos, posto que deixar abertos os portos 칠 un importante risco de seguridade):

**Xerais**:

 - SSH: 22 (TCP).

**Apache Hadoop**:

 - HADOOP_HDFS: 9000 (TCP)
 - HADDOP_MASTER_WEB: 9870 (TCP)
 - HADOOP_SECONDARY_NAMENODE_WEB 9868 (TCP)

**Apache Spark**:

 - SPARK_MAIN: 9001 (TCP)
 - INFO_MASTER_SPARK: 8080 (TCP)
 - INFO_WORKERS_SPARK: 8081 (TCP)
 - INFO_WORKERS_SPARK_WEB: 4040 (TCP)
 - OUTROS: 7707 (TCP)


## Instalaci칩n de ClusterShell

Debemos instalar [ClusterShell](https://clustershell.readthedocs.io/en/latest/) no nodo master ou no equipo onde fagamos a instalaci칩n.

Executemos os comandos dende o noso equipo local ou dende o nodo master, podemos incluir o nodo master para conectarnos contra el mesmo e executar tam칠n nel os comandos.

### Instalaci칩n dende Rocky Linux ou distro baseada en Redhat

`sudo yum --enablerepo=extras -y install epel-release`

`sudo yum install -y clustershell`

### Instalaci칩n dende Linux Mint ou distro baseada en Ubuntu/Debian

`sudo apt-get install clustershell`

## Conexi칩n aos servidores por nome

Deberemos configurar o arquivo `/etc/hosts` cos nomes dos servidores:

~~~~
    X.Y.Z.T1 hadoop1 hadoop1.local
    X.Y.Z.T2 hadoop2 hadoop2.local
    X.Y.Z.T3 hadoop3 hadoop3.local
    ...
    [X.Y.Z.Tn hadoopN hadoopN.local]
~~~~

Cada un destes servidores debe ter como m칤nimo 4 GB de RAM (8 칠 o m칤nimo recomendable m치is imos aplicar restricci칩ns na configuraci칩n).

Debemos ter creado un usuario chamado `cesgaxuser`, co que executaremos t칩dolos comandos. Este usuario existe por defecto nas m치quinas virtuais novas creadas no OpenStack do CESGA.

Asemade, dende o nodo **master** (*hadoop1*) debemos poder conectar por SSH ao resto de nodos: *hadoop2*, *hadoop3* ... *hadoopN* (ter copiada a parte p칰blica da chave SSH no `.ssh/authorized_keys`). Podemos xerar e meter unha chave nova no master (empregando `ssh-keygen`) ou ben copiar a chave que nos xenere o OpenStack (arquivo .pem coa chave RSA).

Se copiamos a chave RSA xerada polo OpenStack durante a creaci칩n da m치quina, podemos copiala mediante scp:

Neste exemplo imaxinamos que este arquivo, gardado no directorio onde nos atopemos, ch치mase: `ficheiro-chave-ssh.pem`

- Copiamos o arquivo (ollo, hai que repetir o arquivo, unha vez para pasalo como chave a empregar e outra para copialo):

    `scp -i ficheiro-chave-ssh.pem ficheiro-chave-ssh.pem cesgaxuser@hadoop1:/home/cesgaxuser/`

Se nos devolve un erro coma este:

~~~~
    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    @         WARNING: UNPROTECTED PRIVATE KEY FILE!          @
    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    Permissions 0755 for 'ficheiro-chave-ssh.pem' are too open.
    It is required that your private key files are NOT accessible by others.
    This private key will be ignored.
    Load key "ficheiro-chave-ssh.pem": bad permissions
~~~~

Deberemos dar permisos adecuados (s칩 podemos ler a chave nos):

`chmod 0600 ficheiro-chave-ssh.pem`

- Conectamos co master:

    `ssh -i ficheiro-chave-ssh.pem cesgaxuser@hadoop1`

- Dentro do master, forzaremos a creaci칩n da estrutura `.ssh` no directorio `$HOME`:

    `ssh localhost`

Preguntaranos se queremos conectar co servidor, xa que 칠 a primeira vez e non co침ece a chave:

    [cesgaxuser@hadoop1 ~]$ ssh localhost
    The authenticity of host 'localhost (::1)' can't be established.
    ECDSA key fingerprint is SHA256:5aeqrZspd4Wev7IrUFH/KS8OXORpa614OEWXRHUG+yE.
    Are you sure you want to continue connecting (yes/no/[fingerprint])? 

Contestamos `yes` e damos a enter para que se cree a estrutura `.ssh`.

Agora toca mover a chave privada ao directorio por defecto:

`mv ficheiro-chave-ssh.pem .ssh/id_rsa`

E dar os permisos adecuados:

`chmod 0600 .ssh/id_rsa`

Finalmente, xeraremos a parte p칰blica da nosa chave privada:

`ssh-keygen -y -f .ssh/id_rsa > .ssh/id_rsa.pub`

Copiaremos as chaves do nodo master ao resto de nodos por comodidade, de xeito que poderemos enviar un comando a t칩dolos nodos dende calquera. Esto pode supor un risco de seguridade, mais poderemos borrar se queremos a chave privada do resto de nodos. De t칩dolos xeitos, tendo en conta que a configuraci칩n 칠 a mesma en t칩dolos nodos e que est치n conectados, que logre acceder a un, moi probablemente poida acceder a todos.

No `.ssh/known_hosts` de *hadoop1*, *hadoop2* ... *hadoopN* precisamos os fingerprints de todos os servidores. Para facelo:

- Descargamos os fingerprints dos servidores (con t칩dalas IPs e nomes):

    `for servidor in $(cat /etc/hosts|grep hadoop); do ssh-keyscan -H $servidor; done >> /home/cesgaxuser/.ssh/known_hosts`

Se mud치semos a li침a que lista os hosts por esta: `$(cat /etc/hosts|grep hadoop|awk '{print $1}'`, poder칤amos asociar as chaves s칩 os enderezos IP.

- Copiamos a configuraci칩n a t칩dolos nodos:

    `clush -l cesgaxuser -bw hadoop[2-4] --copy /home/cesgaxuser/.ssh --dest /home/cesgaxuser/`

### Actualizaci칩n de paquetes

Antes de seguir, 칠 moi conveniente actualizar o sistema para evitar erros de seguridade. Deste xeito probamos que podemos conectar con t칩dolos nodos.

`clush -l cesgaxuser -bw hadoop[1-4] sudo dnf update -y`

(**opcional, m치is recomendable**) E por comodidade, instalamos `nano` un editor moi simple e m치is amigable que `vi` e tam칠n `net-tools` por se precisamos ver certas configuraci칩ns de rede para diagnosticar erros.

`clush -l cesgaxuser -bw hadoop[1-4] sudo dnf install -y nano net-tools`


### Instalaci칩n de Java OpenJDK Correto de Amazon

En t칩dolos equipos debemos ter instalada a m치quina virtual de Java OpenJDK versi칩n Corrreto de Amazon a trav칠s de repositorio. 

Engadimos o respositorio:

`clush -l cesgaxuser -bw hadoop[1-4] sudo curl -L -o /etc/yum.repos.d/corretto.repo https://yum.corretto.aws/corretto.repo`

Instalamos o paquete:

`clush -l cesgaxuser -bw hadoop[1-4] sudo dnf install -y java-11-amazon-corretto-devel`


## Configuraci칩n do **nodo master**
Debemos conectarnos por SSH ao nodo master:

`ssh cesgaxuser@hadoop1`

### Configuraci칩n de JAVA_HOME en t칩dolos servidores

Editaremos o arquivo `/home/cesgaxuser/.bashrc` para configurar as variables de contorna necesarias.

`nano .bashrc`

Metemos estas li침as ao final:

~~~~
    JAVA_HOME='/usr/lib/jvm/java-11-amazon-corretto/'
    export JAVA_HOME
~~~~

Esto configura a variable de contorna `JAVA_HOME`. Non deber칤a ser necesario ao estar instalada a m치quina dende un repositorio automatizado, pero 칠 unha boa pr치ctica para que o resto de scripts non dean fallos ou avisos.

Pechamos a sesi칩n (Ctrl+D) e volvemos entrar (ou executamos `. ./bashrc` ou `source .bashrc` para recargar as variables de entorno).

Copiamos este arquivo ao resto de nodos, empregaremos *clustershell* para simplificar o proceso:

`clush -l cesgaxuser -bw hadoop[2-4] --copy .bashrc --dest /home/cesgaxuser/`


### Descargando e instalando Apache Hadoop en t칩dolos nodos

No nodo master, **descargamos** *Apache Hadoop* da web oficial:

`curl https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz --output hadoop-3.2.4.tar.gz`

E **copi치molo** ao resto de nodos:

`clush -l cesgaxuser -bw hadoop[2-4] --copy hadoop-3.2.4.tar.gz --dest /home/cesgaxuser/`

**Descomprimimos** en t칩dolos nodos:

`clush -l cesgaxuser -bw hadoop[1-4] tar -xzf hadoop-3.2.4.tar.gz`

### Configurando as variables de contorna espec칤ficas de **Apache Hadoop**

Metemos no `.bashrc` ao final as seguintes novas variables da contorna:

~~~~
    HADOOP_HOME='/home/cesgaxuser/hadoop-3.2.4'
    PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
    export HADOOP_HOME
    export PATH
~~~~

E copiamos o arquivo ao resto de nodos para aplicar a mesma configuraci칩n:

`clush -l cesgaxuser -bw hadoop[2-3] --copy .bashrc --dest /home/cesgaxuser/`

Pechamos a sesi칩n (Ctrl+D) e volvemos entrar (ou executamos `. ./bashrc` ou `source .bashrc` para recargar as variables de entorno).

Para comprobar se funciona a instalaci칩n b치sica e as variables de contorna, podemos escribir o comando:

`hdfs`

E deber칤a devolvernos a axuda do comando.

### Configurando Apache Hadoop

Editamos o arquivo de configuraci칩n: `hadoop-3.2.4/etc/hadoop/core-site.xml`

E o deixamos as칤:

~~~~
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://hadoop1:9000</value>
        </property>
    </configuration>
~~~~

Editamos o arquivo `hadoop-3.2.4/etc/hadoop/hdfs-site.xml`:

E o deixamos as칤:
-----------------------------------

~~~~
    <configuration>
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>/home/cesgaxuser/data/nameNode</value>
        </property>

        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/home/cesgaxuser/data/dataNode</value>
        </property>

        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
    </configuration>
~~~~

Agora precisamos crear os directorios que almacenar치n os datos:

 - `mkdir -p /home/cesgaxuser/data/nameNode`
 - `mkdir -p /home/cesgaxuser/data/dataNode`

Editamos o arquivo: `hadoop-3.2.4/etc/hadoop/mapred-site.xml`

E deix치molo as칤:

~~~~
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
        <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
    
    <!-- L칤mite de uso de RAM (non po침er se temos alomenos 8GB por servidor -->

    <property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
    </property>

    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>256</value>
    </property>

    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
    </property>

    </configuration>
~~~~

Editamos o arquivo: `hadoop-3.2.4/etc/hadoop/yarn-site.xml` e metemos a seguinte configuraci칩n (ollo de subtituir XXX.XXX.XXX.XXX pola IP do nodo master):

~~~~
    <configuration>
        <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
        </property>

        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>XXX.XXX.XXX.XXX</value>
        </property>

        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>

        <!-- L칤mite de uso de RAM (non po침er se temos alomenos 8GB por servidor -->
        <property>
            <name>yarn.nodemanager.resource.memory-mb</name>
            <value>1536</value>
        </property>
        
        <property>
            <name>yarn.scheduler.maximum-allocation-mb</name>
            <value>1536</value>
        </property>
    
        <property>
            <name>yarn.scheduler.minimum-allocation-mb</name>
            <value>128</value>
        </property>
    
        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>

    </configuration>
~~~~

E tam칠n metemos os nodos no arquivo: `hadoop-3.2.4/etc/hadoop/workers`:

~~~~
    hadoop2
    hadoop3
    hadoop4
~~~~

Con esto ter칤amos configurado **yarn** en *hadoop1* (master).

### Configuraci칩n do resto de nodos

Podemos copiar simplemente a configuraci칩n ao resto de nodos:

`clush -l cesgaxuser -bw hadoop[2-3] --copy hadoop-3.2.4/etc/hadoop/workers hadoop-3.2.4/etc/hadoop/yarn-site.xml hadoop-3.2.4/etc/hadoop/mapred-site.xml hadoop-3.2.4/etc/hadoop/hdfs-site.xml hadoop-3.2.4/etc/hadoop/core-site.xml --dest /home/cesgaxuser/hadoop-3.2.4/etc/hadoop`

### Formatear o HDFS:

Dende *hadoop1* (o nodo master) executamos:

`hdfs namenode -format`

E finalmente dende o master iniciamos todo o sistema (esto conecta por SSH aos nodos e executa os comandos necesarios para que se po침an a traballar):

`start-dfs.sh`

### Arrancando Apache Hadoop ao inicio con cron

Se queremos lanzar o proceso de Apache Hadoop dende cron (por exemplo para facer uso da opci칩n `@reboot`) deberemos cambiar por si acaso o arquivo: `hadoop-3.2.4/etc/hadoop/hadoop-env.sh` e mudar as seguintes variables, para non depender do .bashrc do usuario:

~~~~
    export JAVA_HOME='/usr/lib/jvm/java-11-amazon-corretto/'
    export HADOOP_HOME='/home/cesgaxuser/hadoop-3.2.4'
    export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    #HADOOP_OPTS=
~~~~

Unha vez cambiemos o arquivo, copiar칠molo ao resto de nodos, por coherencia e por manter igual a configuraci칩n en t칩dolos sitios.

`clush -l cesgaxuser -bw hadoop[2-3] --copy hadoop-3.2.4/etc/hadoop/hadoop-env.sh --dest /home/cesgaxuser/hadoop-3.2.4/etc/hadoop`

Finalmente editaremos o cron do nodo master para indicarlle que arranque Hadoop cando iniciemos a m치quina:

(**importante**: *Se non queres empregar vi, establece a variable `EDITOR=nano` se o tes instalado*)

 `crontab -e`

E metemos no arquivo o seguinte contido:

~~~~
    @reboot /home/cesgaxuser/hadoop-3.2.4/sbin/start-dfs.sh
~~~~

Por 칰ltimo reiniciamos t칩dolos nodos:

`clush -l cesgaxuser -bw hadoop[1-4] sudo reboot`

### Comprobando que funciona

Para obter informaci칩n do HDFS, podemos empregar o comando:

`hdfs dfsadmin -report`

Queda iniciar yarn.

## Bibliograf칤a

Podes atopar m치is informaci칩n:

 - <https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/>
 - <https://sparkbyexamples.com/spark/spark-setup-on-hadoop-yarn/>
 - <https://www.linode.com/docs/guides/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/>
 - <https://www.tutorialspoint.com/es/hadoop/hadoop_multi_node_cluster.htm>
 - <https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml>

## Copyright

춸 2023 - Jose S치nchez

Se atopas erros, agradecerei que me env칤es un email/mensaxe co aviso ou as correcci칩ns.
